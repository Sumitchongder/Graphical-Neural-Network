{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e0ef0d7-ff57-4524-ba00-94f45b8936cb",
   "metadata": {},
   "source": [
    "## Environment setup, logging, and reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc562acc-53f7-44a9-9dea-66e643b1c0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core\n",
    "import os, math, random, time, json, logging\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Torch / GNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "\n",
    "# HuggingFace LLM stack\n",
    "# pip install transformers datasets peft optuna\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    Trainer, TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# HPO\n",
    "import optuna\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"[%(asctime)s] %(levelname)s - %(message)s\")\n",
    "LOGGER = logging.getLogger(\"hybrid_gnn_llm\")\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    seed: int = 42\n",
    "    gnn_hidden: int = 64\n",
    "    gnn_out: int = 32\n",
    "    n_nodes: int = 200\n",
    "    graph_type: str = \"barabasi_albert\"\n",
    "    graph_param: int = 3           # BA graph: m parameter\n",
    "    node_feature_dim: int = 16\n",
    "    n_classes: int = 3\n",
    "    train_ratio: float = 0.8\n",
    "\n",
    "    model_name: str = \"sshleifer/tiny-gpt2\"  # demo model\n",
    "    block_size: int = 128\n",
    "    default_lr: float = 2e-4\n",
    "    default_batch_size: int = 16\n",
    "    default_num_train_epochs: int = 3\n",
    "    use_lora: bool = True\n",
    "    lora_r: int = 8\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    mixed_precision: str = \"fp16\"\n",
    "    output_dir: str = \"./hybrid_artifacts\"\n",
    "\n",
    "CFG = Config()\n",
    "\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "os.makedirs(CFG.output_dir, exist_ok=True)\n",
    "set_all_seeds(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712c3add-55a7-427a-a68e-ae1d24e64a16",
   "metadata": {},
   "source": [
    "## Synthetic graph generation and node features\n",
    "\n",
    "We simulate a social-like graph (Barabási–Albert preferential attachment), assign random node features, and create labels (e.g., community or role classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "433f9a19-87d9-4726-a087-80a83140f0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-26 21:09:24,133] INFO - Graph nodes: 200, edges: 591\n"
     ]
    }
   ],
   "source": [
    "def generate_graph(n_nodes: int, graph_type: str, param: int) -> nx.Graph:\n",
    "    if graph_type == \"barabasi_albert\":\n",
    "        G = nx.barabasi_albert_graph(n=n_nodes, m=param, seed=CFG.seed)\n",
    "    else:\n",
    "        G = nx.erdos_renyi_graph(n=n_nodes, p=0.05, seed=CFG.seed)\n",
    "    return G\n",
    "\n",
    "def make_node_features_labels(G: nx.Graph, feat_dim: int, n_classes: int):\n",
    "    n = G.number_of_nodes()\n",
    "    X = np.random.randn(n, feat_dim).astype(np.float32)\n",
    "    # Labels derived from degree buckets (proxy for role classes)\n",
    "    degrees = np.array([d for _, d in G.degree()], dtype=np.int32)\n",
    "    bins = np.quantile(degrees, [0.33, 0.66])\n",
    "    y = np.zeros(n, dtype=np.int64)\n",
    "    y[degrees > bins[0]] = 1\n",
    "    y[degrees > bins[1]] = 2\n",
    "    return X, y\n",
    "\n",
    "G = generate_graph(CFG.n_nodes, CFG.graph_type, CFG.graph_param)\n",
    "X, y = make_node_features_labels(G, CFG.node_feature_dim, CFG.n_classes)\n",
    "LOGGER.info(f\"Graph nodes: {G.number_of_nodes()}, edges: {G.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8be202-d8fc-4ea1-8225-bcdac358ce63",
   "metadata": {},
   "source": [
    "## Lightweight GNN: GraphSAGE-style aggregator (PyTorch)\n",
    "\n",
    "We avoid external GNN libs to keep it portable. This GraphSAGE-like layer aggregates neighbor features by mean and applies linear transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8355ca6-94b4-4418-a10d-2c2118796014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanAggregator(nn.Module):\n",
    "    def forward(self, x: torch.Tensor, adj_lists: List[List[int]]) -> torch.Tensor:\n",
    "        # x: [N, D], adj_lists: list of neighbors per node\n",
    "        N, D = x.shape\n",
    "        out = torch.zeros_like(x)\n",
    "        for i in range(N):\n",
    "            nbrs = adj_lists[i]\n",
    "            if len(nbrs) == 0:\n",
    "                out[i] = x[i]\n",
    "            else:\n",
    "                out[i] = x[nbrs].mean(dim=0)\n",
    "        return out\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: int, out_dim: int, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.agg1 = MeanAggregator()\n",
    "        self.lin1 = nn.Linear(in_dim + in_dim, hidden)\n",
    "        self.agg2 = MeanAggregator()\n",
    "        self.lin2 = nn.Linear(hidden + hidden, out_dim)\n",
    "        self.cls  = nn.Linear(out_dim, n_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, adj_lists: List[List[int]]):\n",
    "        # Layer 1\n",
    "        nbr1 = self.agg1(x, adj_lists)\n",
    "        h1 = torch.cat([x, nbr1], dim=1)\n",
    "        h1 = F.relu(self.lin1(h1))\n",
    "\n",
    "        # Layer 2\n",
    "        nbr2 = self.agg2(h1, adj_lists)\n",
    "        h2 = torch.cat([h1, nbr2], dim=1)\n",
    "        h2 = F.relu(self.lin2(h2))\n",
    "\n",
    "        logits = self.cls(h2)\n",
    "        return h2, logits\n",
    "\n",
    "def build_adj_lists(G: nx.Graph) -> List[List[int]]:\n",
    "    return [list(G.neighbors(i)) for i in range(G.number_of_nodes())]\n",
    "\n",
    "adj_lists = build_adj_lists(G)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_gnn = GraphSAGE(CFG.node_feature_dim, CFG.gnn_hidden, CFG.gnn_out, CFG.n_classes).to(device)\n",
    "opt = torch.optim.Adam(model_gnn.parameters(), lr=1e-3)\n",
    "\n",
    "X_t = torch.from_numpy(X).to(device)\n",
    "y_t = torch.from_numpy(y).to(device)\n",
    "\n",
    "# Train/eval split\n",
    "idx = np.arange(CFG.n_nodes)\n",
    "np.random.shuffle(idx)\n",
    "split = int(CFG.train_ratio * CFG.n_nodes)\n",
    "train_idx, eval_idx = idx[:split], idx[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573fea50-87ec-4e2b-8206-0538aa0db99d",
   "metadata": {},
   "source": [
    "## Train GNN to produce embeddings and classify nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ca3d3dd-2797-4a3f-8456-de4cdc6c8ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-26 21:11:00,576] INFO - [GNN] Epoch 10 | loss=1.0775 | eval_acc=0.325\n",
      "[2025-11-26 21:11:01,097] INFO - [GNN] Epoch 20 | loss=1.0413 | eval_acc=0.300\n",
      "[2025-11-26 21:11:01,577] INFO - [GNN] Epoch 30 | loss=0.9923 | eval_acc=0.350\n",
      "[2025-11-26 21:11:02,037] INFO - [GNN] Epoch 40 | loss=0.9195 | eval_acc=0.325\n",
      "[2025-11-26 21:11:02,056] INFO - GNN embeddings shape: (200, 32)\n"
     ]
    }
   ],
   "source": [
    "def train_gnn(epochs=50):\n",
    "    model_gnn.train()\n",
    "    for ep in range(epochs):\n",
    "        opt.zero_grad()\n",
    "        h, logits = model_gnn(X_t, adj_lists)\n",
    "        loss = F.cross_entropy(logits[train_idx], y_t[train_idx])\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if (ep+1) % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                _, logits_eval = model_gnn(X_t, adj_lists)\n",
    "                pred = logits_eval.argmax(dim=1).cpu().numpy()\n",
    "                acc = (pred[eval_idx] == y[eval_idx]).mean()\n",
    "            LOGGER.info(f\"[GNN] Epoch {ep+1:02d} | loss={loss.item():.4f} | eval_acc={acc:.3f}\")\n",
    "\n",
    "train_gnn(epochs=40)\n",
    "\n",
    "model_gnn.eval()\n",
    "with torch.no_grad():\n",
    "    H, logits = model_gnn(X_t, adj_lists)  # H: node embeddings [N, out_dim]\n",
    "H_np = H.detach().cpu().numpy()\n",
    "LOGGER.info(f\"GNN embeddings shape: {H_np.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497e7d20-fc1c-4eea-a7e9-e37073532d37",
   "metadata": {},
   "source": [
    "## Graph-to-text conversion: prompts from GNN embeddings and topology\n",
    "\n",
    "We convert structural info + learned embeddings into natural language instructions for LLM fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d60db8-bf0e-4615-9d11-539a1892fceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-26 21:12:44,921] INFO - Graph-to-text pairs -> train=160, eval=40\n"
     ]
    }
   ],
   "source": [
    "def describe_node(i: int, G: nx.Graph, emb: np.ndarray) -> str:\n",
    "    deg = G.degree(i)\n",
    "    nbrs = list(G.neighbors(i))\n",
    "    emb_summary = \", \".join([f\"{v:.2f}\" for v in emb[i][:6]])  # small preview\n",
    "    return (\n",
    "        f\"Node {i} has degree {deg} and neighbors {nbrs}. \"\n",
    "        f\"Embedding summary: [{emb_summary}]\"\n",
    "    )\n",
    "\n",
    "def instruction_from_graph(i: int, G: nx.Graph, emb: np.ndarray) -> str:\n",
    "    deg = G.degree(i)\n",
    "    if deg >= 5:\n",
    "        task = \"Summarize\"\n",
    "        guidance = \"Provide a concise summary of this node's role and influence.\"\n",
    "    elif deg >= 3:\n",
    "        task = \"Classify\"\n",
    "        guidance = \"Classify the node into High/Medium/Low influence.\"\n",
    "    else:\n",
    "        task = \"Extract\"\n",
    "        guidance = \"Extract notable structural properties (degree, clustering).\"\n",
    "    desc = describe_node(i, G, emb)\n",
    "    return f\"{task}: {guidance} Context: {desc}\"\n",
    "\n",
    "def response_stub(instruction: str) -> str:\n",
    "    # Placeholder expected output forms; in fine-tuning, model learns to produce these patterns\n",
    "    if instruction.startswith(\"Summarize\"):\n",
    "        return \"Summary: Node exhibits high connectivity and centrality.\"\n",
    "    elif instruction.startswith(\"Classify\"):\n",
    "        return \"Class: Medium influence.\"\n",
    "    else:\n",
    "        return \"Properties: degree=low; clustering=moderate.\"\n",
    "\n",
    "pairs = [{\"instruction\": instruction_from_graph(i, G, H_np), \"response\": response_stub(instruction_from_graph(i, G, H_np))}\n",
    "         for i in range(CFG.n_nodes)]\n",
    "\n",
    "train_pairs = pairs[:int(0.8*len(pairs))]\n",
    "eval_pairs  = pairs[int(0.8*len(pairs)):]\n",
    "train_ds = Dataset.from_list(train_pairs)\n",
    "eval_ds  = Dataset.from_list(eval_pairs)\n",
    "LOGGER.info(f\"Graph-to-text pairs -> train={len(train_ds)}, eval={len(eval_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835d7455-0256-4af6-b943-61f4409e4345",
   "metadata": {},
   "source": [
    "## Tokenization and LoRA PEFT setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19255f71-98a3-4696-b6be-3a712b00b0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51f553e683c4f8487d14725ebf83af8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078e58f6b2274003b24d3e52a78dae96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63be03c263364d988a91f092a9b24600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0052fb95263941ec93418cd0362f6a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumit\\anaconda3\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "[2025-11-26 21:14:26,773] INFO - LLM ready with LoRA adapters\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def fmt(ex):\n",
    "    return {\"text\": f\"Instruction: {ex['instruction']}\\nResponse: {ex['response']}\"}\n",
    "\n",
    "train_fmt = train_ds.map(fmt)\n",
    "eval_fmt  = eval_ds.map(fmt)\n",
    "\n",
    "def tok(ex):\n",
    "    return tokenizer(ex[\"text\"], truncation=True, max_length=CFG.block_size, padding=\"max_length\")\n",
    "\n",
    "train_tok = train_fmt.map(tok, batched=True, remove_columns=train_fmt.column_names)\n",
    "eval_tok  = eval_fmt.map(tok, batched=True, remove_columns=eval_fmt.column_names)\n",
    "collator  = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(CFG.model_name)\n",
    "if CFG.use_lora:\n",
    "    peft_cfg = LoraConfig(\n",
    "        r=CFG.lora_r, lora_alpha=CFG.lora_alpha, lora_dropout=CFG.lora_dropout,\n",
    "        bias=\"none\", target_modules=[\"c_attn\", \"c_proj\"]\n",
    "    )\n",
    "    model = get_peft_model(base_model, peft_cfg)\n",
    "else:\n",
    "    model = base_model\n",
    "\n",
    "LOGGER.info(\"LLM ready with LoRA adapters\" if CFG.use_lora else \"LLM ready for full fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05dac1b-3609-4ba7-a929-3988181339f4",
   "metadata": {},
   "source": [
    "## Adaptive workload allocation and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42a62834-46b9-4754-9cf5-b2667bcf0404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-26 21:19:57,355] INFO - Device info: {'use_cuda': False, 'n_gpu': 0}\n",
      "[2025-11-26 21:19:57,356] INFO - Adaptive global batch size: 4\n",
      "C:\\Users\\Sumit\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>10.691100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>10.691300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumit\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-26 21:20:09,540] INFO - Evaluation metrics: {'eval_loss': 10.67523193359375, 'eval_runtime': 0.3852, 'eval_samples_per_second': 103.855, 'eval_steps_per_second': 25.964, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "dev_info = detect_devices()\n",
    "LOGGER.info(f\"Device info: {dev_info}\")\n",
    "\n",
    "effective_bs = adaptive_bs(CFG.default_batch_size, dev_info[\"n_gpu\"])\n",
    "LOGGER.info(f\"Adaptive global batch size: {effective_bs}\")\n",
    "\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=CFG.output_dir,\n",
    "    per_device_train_batch_size=max(1, effective_bs // max(1, dev_info[\"n_gpu\"] or 1)),\n",
    "    per_device_eval_batch_size=max(1, effective_bs // max(1, dev_info[\"n_gpu\"] or 1)),\n",
    "    num_train_epochs=CFG.default_num_train_epochs,\n",
    "    learning_rate=CFG.default_lr,\n",
    "    logging_steps=50,\n",
    "    save_steps=200,\n",
    "    report_to=\"none\",              # disable reporting to WandB/Comet\n",
    "    fp16=(CFG.mixed_precision == \"fp16\"),\n",
    "    bf16=(CFG.mixed_precision == \"bf16\"),\n",
    "    warmup_ratio=0.05\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=eval_tok,\n",
    "    data_collator=collator\n",
    ")\n",
    "\n",
    "# ✅ Manual evaluation call after training\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "LOGGER.info(f\"Evaluation metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed79b59c-08a4-4528-91f7-3252999f52f0",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization (Optuna)\n",
    "\n",
    "We optimize learning rate, LoRA rank, and block size to demonstrate infra + efficiency mindset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66b22621-fa51-4ab9-a93f-33426d59650c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "    # Hyperparameters to tune\n",
    "    lr = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-4)\n",
    "    lora_r = trial.suggest_int(\"lora_r\", 4, 32, step=4)\n",
    "    block_size = trial.suggest_int(\"block_size\", 64, 256, step=32)\n",
    "\n",
    "    # Retokenize with new block size\n",
    "    def tok_bs(ex):\n",
    "        return tokenizer(ex[\"text\"], truncation=True, max_length=block_size, padding=\"max_length\")\n",
    "    train_tok_bs = train_fmt.map(tok_bs, batched=True, remove_columns=train_fmt.column_names)\n",
    "    eval_tok_bs  = eval_fmt.map(tok_bs, batched=True, remove_columns=eval_fmt.column_names)\n",
    "\n",
    "    # Rebuild model with supported LoRA targets for GPT-2 style models\n",
    "    base_model_hp = AutoModelForCausalLM.from_pretrained(CFG.model_name)\n",
    "\n",
    "    # IMPORTANT: Only target supported leaf modules (not the container \"attn\")\n",
    "    peft_cfg = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=CFG.lora_alpha,\n",
    "        lora_dropout=CFG.lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,     # ensure correct PEFT task type\n",
    "        target_modules=[\"c_attn\", \"c_proj\"]  # ✅ supported for GPT-2 blocks\n",
    "        # If you want final head adaptation too: add \"lm_head\" (if present in your model)\n",
    "    )\n",
    "    model_hp = get_peft_model(base_model_hp, peft_cfg)\n",
    "\n",
    "    train_args_hp = TrainingArguments(\n",
    "        output_dir=os.path.join(CFG.output_dir, f\"optuna_trial_{trial.number}\"),\n",
    "        per_device_train_batch_size=max(1, effective_bs // max(1, dev_info[\"n_gpu\"] or 1)),\n",
    "        per_device_eval_batch_size=max(1, effective_bs // max(1, dev_info[\"n_gpu\"] or 1)),\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=lr,\n",
    "        logging_steps=50,\n",
    "        # For older Transformers versions, remove scheduling and evaluate manually:\n",
    "        # evaluation_strategy=\"steps\", eval_steps=200,\n",
    "        report_to=\"none\",\n",
    "        fp16=(CFG.mixed_precision == \"fp16\"),\n",
    "        bf16=(CFG.mixed_precision == \"bf16\")\n",
    "    )\n",
    "\n",
    "    trainer_hp = Trainer(\n",
    "        model=model_hp,\n",
    "        args=train_args_hp,\n",
    "        train_dataset=train_tok_bs,\n",
    "        eval_dataset=eval_tok_bs,\n",
    "        data_collator=collator\n",
    "    )\n",
    "\n",
    "    trainer_hp.train()\n",
    "    metrics = trainer_hp.evaluate()  # manual evaluation call (version-safe)\n",
    "    trial.set_user_attr(\"metrics\", metrics)\n",
    "    return metrics.get(\"eval_loss\", float(\"inf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4226adb7-4a45-4f8e-bfe1-d69526b507b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-26 21:24:31,023] A new study created in memory with name: no-name-48375583-77cc-4307-a21f-12af9dac6168\n",
      "[2025-11-26 21:24:31,024] INFO - Starting Optuna hyperparameter search (demo trials)\n",
      "C:\\Users\\Sumit\\AppData\\Local\\Temp\\ipykernel_39568\\187104344.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a6cff141f54bb7b5b857a6eb1837c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97522d5a59f14ff6aefe31901b97c142",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumit\\anaconda3\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumit\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-26 21:24:36,829] Trial 0 finished with value: 10.675223350524902 and parameters: {'learning_rate': 0.0002984108051855515, 'lora_r': 4, 'block_size': 128}. Best is trial 0 with value: 10.675223350524902.\n",
      "C:\\Users\\Sumit\\AppData\\Local\\Temp\\ipykernel_39568\\187104344.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352b840ce51d40209ac5ce2df778b0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be787c7719d437c8c7dd4c3282a1229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumit\\anaconda3\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumit\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-26 21:24:40,612] Trial 1 finished with value: 10.656453132629395 and parameters: {'learning_rate': 5.512699912537529e-05, 'lora_r': 28, 'block_size': 64}. Best is trial 1 with value: 10.656453132629395.\n",
      "C:\\Users\\Sumit\\AppData\\Local\\Temp\\ipykernel_39568\\187104344.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402d045d203042e49252e49f153f7ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3b03b946d2494a8261f3523f5da9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumit\\anaconda3\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumit\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-26 21:24:47,058] Trial 2 finished with value: 10.675196647644043 and parameters: {'learning_rate': 0.00047073541391392024, 'lora_r': 12, 'block_size': 160}. Best is trial 1 with value: 10.656453132629395.\n",
      "C:\\Users\\Sumit\\AppData\\Local\\Temp\\ipykernel_39568\\187104344.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2348b7078e47edb64eb574c73637c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73fd2e251f5436db62715a76f0ecd72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumit\\anaconda3\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumit\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-26 21:24:52,674] Trial 3 finished with value: 10.67529582977295 and parameters: {'learning_rate': 2.4416759036612658e-05, 'lora_r': 12, 'block_size': 128}. Best is trial 1 with value: 10.656453132629395.\n",
      "C:\\Users\\Sumit\\AppData\\Local\\Temp\\ipykernel_39568\\187104344.py:5: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  lr = trial.suggest_loguniform(\"learning_rate\", 1e-5, 5e-4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab90494dd2f42ffa7be0278bb2e67ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1184b6e807b544d792e2f478025d317c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumit\\anaconda3\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 00:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumit\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-26 21:24:58,964] Trial 4 finished with value: 10.675294876098633 and parameters: {'learning_rate': 2.0087504355424445e-05, 'lora_r': 32, 'block_size': 160}. Best is trial 1 with value: 10.656453132629395.\n",
      "[2025-11-26 21:24:58,965] INFO - Best trial: 1, value=10.656453132629395, params={'learning_rate': 5.512699912537529e-05, 'lora_r': 28, 'block_size': 64}\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "LOGGER.info(\"Starting Optuna hyperparameter search (demo trials)\")\n",
    "study.optimize(objective, n_trials=5)\n",
    "LOGGER.info(f\"Best trial: {study.best_trial.number}, value={study.best_trial.value}, params={study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d580328-fd74-4c9b-b348-50f8109e86ec",
   "metadata": {},
   "source": [
    "## Train final model with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "911a4f0e-e7d4-44c8-a75b-960cd28f4be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-26 21:27:05,425] INFO - LoRA candidate targets found: ['transformer.h.0.attn.c_attn', 'transformer.h.0.attn.c_proj', 'transformer.h.0.mlp.c_proj', 'transformer.h.1.attn.c_attn', 'transformer.h.1.attn.c_proj', 'transformer.h.1.mlp.c_proj', 'lm_head']\n",
      "C:\\Users\\Sumit\\anaconda3\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>10.656200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>10.656100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumit\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-26 21:27:12,010] INFO - Final eval metrics: {'eval_loss': 10.656448364257812, 'eval_runtime': 0.2563, 'eval_samples_per_second': 156.053, 'eval_steps_per_second': 39.013, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Inspect module names to confirm valid targets\n",
    "def list_candidate_targets(model):\n",
    "    candidates = []\n",
    "    for name, mod in model.named_modules():\n",
    "        if any(t in name for t in [\"c_attn\", \"c_proj\", \"lm_head\"]):\n",
    "            candidates.append(name)\n",
    "    return candidates\n",
    "\n",
    "base_model_final = AutoModelForCausalLM.from_pretrained(CFG.model_name)\n",
    "\n",
    "# Log candidates so you know what's available in your architecture\n",
    "candidates = list_candidate_targets(base_model_final)\n",
    "LOGGER.info(f\"LoRA candidate targets found: {candidates[:10]}\")\n",
    "\n",
    "# Use supported leaf modules for GPT-2 style architectures\n",
    "peft_cfg_final = LoraConfig(\n",
    "    r=CFG.lora_r,\n",
    "    lora_alpha=CFG.lora_alpha,\n",
    "    lora_dropout=CFG.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,            # ensure correct PEFT route\n",
    "    target_modules=[\"c_attn\", \"c_proj\"]      # ✅ do NOT use \"attn\"\n",
    "    # Optionally include \"lm_head\" if present and you want to adapt the output head:\n",
    "    # target_modules=[\"c_attn\", \"c_proj\", \"lm_head\"]\n",
    ")\n",
    "\n",
    "model_final = get_peft_model(base_model_final, peft_cfg_final)\n",
    "\n",
    "train_args_final = TrainingArguments(\n",
    "    output_dir=os.path.join(CFG.output_dir, \"final\"),\n",
    "    per_device_train_batch_size=max(1, effective_bs // max(1, dev_info[\"n_gpu\"] or 1)),\n",
    "    per_device_eval_batch_size=max(1, effective_bs // max(1, dev_info[\"n_gpu\"] or 1)),\n",
    "    num_train_epochs=CFG.default_num_train_epochs,\n",
    "    learning_rate=CFG.default_lr,\n",
    "    logging_steps=50,\n",
    "    # If your Transformers version is old, omit evaluation scheduling and evaluate manually:\n",
    "    # evaluation_strategy=\"steps\", eval_steps=200,\n",
    "    report_to=\"none\",\n",
    "    fp16=(CFG.mixed_precision == \"fp16\"),\n",
    "    bf16=(CFG.mixed_precision == \"bf16\"),\n",
    "    save_steps=200,\n",
    "    warmup_ratio=0.05\n",
    ")\n",
    "\n",
    "trainer_final = Trainer(\n",
    "    model=model_final,\n",
    "    args=train_args_final,\n",
    "    train_dataset=train_tok_final,\n",
    "    eval_dataset=eval_tok_final,\n",
    "    data_collator=collator\n",
    ")\n",
    "\n",
    "trainer_final.train()\n",
    "final_metrics = trainer_final.evaluate()  # manual evaluation works across versions\n",
    "LOGGER.info(f\"Final eval metrics: {final_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67ad062-141b-4324-aae4-69f15b9676f5",
   "metadata": {},
   "source": [
    "## Agentic hybrid reasoning: use GNN context to condition LLM outputs\n",
    "\n",
    "We route tasks based on node structure and refine with the fine-tuned LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c920c634-a340-4182-877b-42c87606b354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 95 | Tool=summarize\n",
      "Instruction: Summarize: Provide a concise summary of this node's role and influence. Context: Node 95 has degree 7 and neighbors [0, 10, 69, 98, 117, 173, 185]. Embedding summary: [0.00, 0.00, 0.00, 0.01, 0.00, 0.90]\n",
      "LLM Output: Summary: High centrality\n",
      "\n",
      "Node 15 | Tool=summarize\n",
      "Instruction: Summarize: Provide a concise summary of this node's role and influence. Context: Node 15 has degree 9 and neighbors [12, 5, 7, 89, 113, 114, 134, 177, 183]. Embedding summary: [0.00, 0.00, 0.00, 0.28, 0.21, 0.47]\n",
      "LLM Output: Summary: High centrality\n",
      "\n",
      "Node 30 | Tool=summarize\n",
      "Instruction: Summarize: Provide a concise summary of this node's role and influence. Context: Node 30 has degree 7 and neighbors [4, 14, 7, 32, 54, 131, 143]. Embedding summary: [0.00, 0.00, 0.00, 0.21, 0.11, 0.00]\n",
      "LLM Output: Summary: High centrality\n",
      "\n",
      "Node 158 | Tool=classify\n",
      "Instruction: Classify: Classify the node into High/Medium/Low influence. Context: Node 158 has degree 4 and neighbors [62, 50, 134, 196]. Embedding summary: [0.00, 0.00, 0.00, 0.00, 0.02, 0.62]\n",
      "LLM Output: Class: High influence\n",
      "\n",
      "Node 128 | Tool=summarize\n",
      "Instruction: Summarize: Provide a concise summary of this node's role and influence. Context: Node 128 has degree 5 and neighbors [51, 4, 12, 146, 173]. Embedding summary: [0.00, 0.00, 0.09, 0.09, 0.06, 0.20]\n",
      "LLM Output: Summary: High centrality\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def agent_route(inst: str) -> str:\n",
    "    if inst.startswith(\"Summarize\"): return \"summarize\"\n",
    "    if inst.startswith(\"Classify\"):  return \"classify\"\n",
    "    return \"extract\"\n",
    "\n",
    "TOOLS = {\n",
    "    \"summarize\": lambda ctx: \"Summary: \" + (\"High centrality\" if \"degree\" in ctx else \"OK\"),\n",
    "    \"classify\":  lambda ctx: \"Class: High influence\" if \"degree\" in ctx and \"neighbors\" in ctx else \"Class: Low\",\n",
    "    \"extract\":   lambda ctx: \"Properties: degree, clustering, egonet size.\"\n",
    "}\n",
    "\n",
    "def agentic_hybrid(G, H, n_samples=5):\n",
    "    outputs = []\n",
    "    for i in np.random.choice(range(G.number_of_nodes()), size=n_samples, replace=False):\n",
    "        inst = instruction_from_graph(i, G, H)\n",
    "        tool = agent_route(inst)\n",
    "        tool_out = TOOLS[tool](inst)\n",
    "        prompt = f\"Instruction: {inst}\\nToolOutput: {tool_out}\\nResponse:\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        try:\n",
    "            gen = model_final.generate(**inputs, max_length=CFG.block_size)\n",
    "            text = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "        except Exception:\n",
    "            text = tool_out\n",
    "        outputs.append({\"node\": i, \"instruction\": inst, \"tool\": tool, \"llm_output\": text[:300]})\n",
    "    return outputs\n",
    "\n",
    "hybrid_outputs = agentic_hybrid(G, H_np, n_samples=5)\n",
    "for o in hybrid_outputs:\n",
    "    print(f\"Node {o['node']} | Tool={o['tool']}\\nInstruction: {o['instruction']}\\nLLM Output: {o['llm_output']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e373612-9435-4955-ae1f-0ba8674f3692",
   "metadata": {},
   "source": [
    "## RL-style reward shaping for structural fidelity\n",
    "We define a simple reward that checks whether the LLM output mentions expected structural properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fab01f0f-cb3e-4db5-8982-560a725ef8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-11-26 21:29:10,059] INFO - Hybrid RL-style metrics: {'mean_reward': 0.7, 'min_reward': 0.5, 'max_reward': 0.75}\n"
     ]
    }
   ],
   "source": [
    "def reward_structural(text: str) -> float:\n",
    "    reward = 0.0\n",
    "    if \"Summary:\" in text or \"Class:\" in text or \"Properties:\" in text: reward += 0.5\n",
    "    for kw in [\"degree\", \"neighbors\", \"centrality\", \"clustering\"]:\n",
    "        if kw in text: reward += 0.25\n",
    "    return reward\n",
    "\n",
    "def evaluate_hybrid(outputs: List[Dict[str, Any]]):\n",
    "    rewards = [reward_structural(o[\"llm_output\"]) for o in outputs]\n",
    "    return {\n",
    "        \"mean_reward\": float(np.mean(rewards)),\n",
    "        \"min_reward\": float(np.min(rewards)),\n",
    "        \"max_reward\": float(np.max(rewards))\n",
    "    }\n",
    "\n",
    "hybrid_metrics = evaluate_hybrid(hybrid_outputs)\n",
    "LOGGER.info(f\"Hybrid RL-style metrics: {hybrid_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2303c3-a315-42ea-9494-dce7cc0bda5c",
   "metadata": {},
   "source": [
    "## Artifact serialization and report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b491218f-de63-4258-baff-3ee3603c396d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"config\": {\n",
      "    \"seed\": 42,\n",
      "    \"gnn_hidden\": 64,\n",
      "    \"gnn_out\": 32,\n",
      "    \"n_nodes\": 200,\n",
      "    \"graph_type\": \"barabasi_albert\",\n",
      "    \"graph_param\": 3,\n",
      "    \"node_feature_dim\": 16,\n",
      "    \"n_classes\": 3,\n",
      "    \"train_ratio\": 0.8,\n",
      "    \"model_name\": \"sshleifer/tiny-gpt2\",\n",
      "    \"block_size\": 64,\n",
      "    \"default_lr\": 5.512699912537529e-05,\n",
      "    \"default_batch_size\": 16,\n",
      "    \"default_num_train_epochs\": 3,\n",
      "    \"use_lora\": true,\n",
      "    \"lora_r\": 28,\n",
      "    \"lora_alpha\": 16,\n",
      "    \"lora_dropout\": 0.05,\n",
      "    \"mixed_precision\": \"fp16\",\n",
      "    \"output_dir\": \"./hybrid_artifacts\"\n",
      "  },\n",
      "  \"gnn_eval_preview\": {\n",
      "    \"n_nodes\": 200,\n",
      "    \"n_edges\": 591\n",
      "  },\n",
      "  \"final_eval_metrics\": {\n",
      "    \"eval_loss\": 10.656448364257812,\n",
      "    \"eval_runtime\": 0.2563,\n",
      "    \"eval_samples_per_second\": 156.053,\n",
      "    \"eval_steps_per_second\": 39.013,\n",
      "    \"epoch\": 3.0\n",
      "  },\n",
      "  \"hpo_best\": {\n",
      "    \"learning_rate\": 5.512699912537529e-05,\n",
      "    \"lora_r\": 28,\n",
      "    \"block_size\": 64\n",
      "  },\n",
      "  \"hybrid_outputs\": [\n",
      "    {\n",
      "      \"node\": 95,\n",
      "      \"instruction\": \"Summarize: Provide a concise summary of this node's role and influence. Context: Node 95 has degree 7 and neighbors [0, 10, 69, 98, 117, 173, 185]. Embedding summary: [0.00, 0.00, 0.00, 0.01, 0.00, 0.90]\",\n",
      "      \"t\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def to_json_safe(obj):\n",
    "    \"\"\"Recursively convert NumPy types and other non-serializable objects to JSON-safe Python types.\"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: to_json_safe(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [to_json_safe(v) for v in obj]\n",
    "    elif isinstance(obj, tuple):\n",
    "        return tuple(to_json_safe(v) for v in obj)\n",
    "    elif isinstance(obj, (np.integer,)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.floating,)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.ndarray,)):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Apply conversion before dumping\n",
    "report_safe = to_json_safe(report)\n",
    "\n",
    "print(json.dumps(report_safe, indent=2)[:1200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdef58b-6fa2-4fd4-80d8-c5178fa9477b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
